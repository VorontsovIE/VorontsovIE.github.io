---
layout: page
title: Идеи проектов (версия 2023 года)
permalink: /project_ideas/
---

_Старые идеи проектов живут в [этой записи](/2019/09/16/project-ideas-2019-edition.html). Я их буду постепенно переносить сюда._

## Data-исследования и data-инженерия

* Как зависит распределение реакций от того какие появились на посте первыми? Смотреть на одних и тех же телеграм-постах в каналах и репостах.

* Какую долю площади города (по данным Open Street Map) занимают жилые дома, территории заводов, зелёных насаждений. По результатам можно сделать игру-тест: слайдером разделить свой город на доли. И выдавать результат «знаток города NN» для шаринга в соцсетях.

* Как устроены поколенческие разрывы по данным переписок / статуса дружбы в соцсетях. Как это работает в разных сообществах. Например, в науке, откуда в какой-то момент вымыло людей среднего возраста и молодёжь осталась с дедами? Верно ли, что активное участие в одном разновозрастном сообществе сильно увеличивает шанс, что человек будет состоять и в других таких же сообществах?

  > Мы тут с товарищем обсуждали поколенческий разрыв.
  >
  > Он рассказал, что у них в лаборатории разрыв коммуникации почему-то между людьми возраста 30 с небольшим и поколением на пять лет старше. А разрыва с поколением студентов (т.е. 20-летних) не возникает. И предположил, что это потому что вокруг нынешних 35–40-летних в момент их становления учёными были только деды, и они сформировали свои коммуникативные навыки об них.
  >
  > Это, понятное дело, anecdotal evidence. Но интересно, можно ли попытаться эти разрывы найти в переписках. Скажем, если мы приходим на стену какого-нибудь СМИ в соцсети, где незнакомцы комментируют друг друга. Будут ли там общаться 20-летние с 50-летними? Или есть какие-то маркеры возраста, которые делают эту коммуникацию маловероятной? Можем ли мы просто оценить число ответов людей разных возрастных страт на комментарии своей страты и других страт?

* Есть огромный датасет информации про воевавших во второй и первой мировых войнах, взятый с [ОБД Мемориал](https://obd-memorial.ru/) / [Память народа](https://pamyat-naroda.ru/) / [Памяти героев Великой войны 1914–1918](https://gwar.mil.ru/)).
По результатам можно придумать кучу разных исследований. Вот, например, наше исследование того, [как был устроен призыв](https://sysblok.ru/history/neizvestnyj-soldat/) в Великую Отечественную (и чуть ранее).

  С этими датасетами проблема в том, что это очень грязные данные (они дословно внесены с бумажных документов; со всеми опечатками и неточностями). Хотелось бы эти данные обработать, а потом что-нибудь полезное из них извлечь:

    * геокодировать записи. Построить карты расположения, например, госпиталей,
    * сделать record linkage, т.е. объединить записи про одного и того же человека воедино. Сейчас это крайне трудно, т.к. опечатки, разные написания одних и тех же, например, адресов итд.
    * есть ещё миллион задач по скрещиванию этих данных с данными в других биографических БД (типа [базы репрессированных](https://lists.memo.ru/) Международного Мемориала).


## Разработка учебных тренажёров

* Разработка интерактивного учебника по статистике. Задача учебника — научить людей основным понятиям из статистики через программирование. Соответственно нужно что-то типа юпитер-тетрадки с шаблонами кода, с визуализаторами, которые помогают продемонстрировать работу кода. Генераторы распределений, описательные статистики, статистические критерии, марковские процессы итд.

* Математическая IDE. Хотелось бы сделать MVP и потестировать на школьниках.

  > Я отмечаю, что у школьников в среднем всё куда проще с изучением программирования, чем с изучением математики. Моя гипотеза в том, что в программировании есть хорошие инструменты, которые помогают писать код, даже когда ты ничего не знаешь про язык программирования. IDE просто не даст тебе сделать некорректные действия. И заодно даёт мгновенную обратную связь. А после того как ты поработаешь с IDE, у тебя уже появляется встроенное чутьё.
  > 
  > Моя мысль в том, что было бы здорово иметь математичeскую IDE, которая будет помогать преобразовывать математические выражения. Не сама решать, а именно подсказывать допустимые действия и не показывать недопустимые. И демонстрировать строка за строкой цепочку преобразований, как в тетрадке. Ну и разумеется в зависимости от уровня школьника/студента подсказывать более сложные действия. 
  >
  > Скажем 5-класснику она не показывает операцию сложения дробей, пока они не приведены к общему множителю. А студента-инженера уже не мучает подробностями такого рода.

## Исследования в области ML/DL

__Disclaimer:__ Хотя я изучал машинное обучение и обучение глубоких нейросетей, я не являюсь действующим исследователем или практиком в области, поэтому не знаю текущее положение дел науки. Я всего лишь формулировал те вопросы, которые мне самому интересны. По некоторым темам я постарался сделать небольшой обзор литературы, но легко может оказаться, что я пропустил статью, которая уже отвечает на заданные мной вопросы.

* _(задача уже выполняется одним школьником, но верю, что тут есть место для ещё одного человека)_
  При оптимизации функции потерь методом стохастического градиентного спуска мы на каждом шаге строим оценку градиента функции потерь по одному батчу, делаем шаг и считаем градиент из новой точки по новому батчу.

  Каждый батч, фактически, задаёт новую функцию — частичную функцию потерь, которая является грубым приближением исходной функции потерь. И то, что у нас есть разные приближения, позволяет нам выбираться из локальных минимумов. А кроме того, аппроксимацию просто гораздо быстрее считать.

  Сейчас этот процесс параллелится по примерам внутри одного батча. Хотелось бы научиться параллелить его ещё и по разным батчам. Но в исходной постановке это невозможно, потому пока мы не вычислили первый градиент, мы не знаем, градиент в какой точке мы будем считать для второго батча.

  Однако нам ничто не мешает посчитать градиенты по нескольким частичным функциям потерь в одной и той же — начальной — точке. И посмотреть, как мы можем агрегировать эти градиенты. Например, чтобы сделать более длинный шаг. Можно, например, сделать отдельную маленькую нейросеточку, которая по трём градиентам в исходной точке предсказывает, где окажется модель через 3 шага. Ну а дальше надо смотреть, сколько разных градиентов имеет смысл агрегировать, а сколько уже бесполезно.

  Дальше можно придумать, как агрегировать градиенты по отдельным примерам внутри одного батча. Например, не просто суммировать их, а делать суммы по нескольким случайным подмножествам примеров, и агрегировать их нашей процедурой.


* Мне интересно, как устроено пространство параметров нейросети. Какие в нём есть симметрии/инварианты etc. 

  Я знаю, что некоторые люди изучают это методами линейной алгебры и выпуклого анализа. Например, смотрят на ландшафты функции потерь при оверпараметризации итд. В том числе что-то есть в курсе теории дип-лёнинга, TDL ([часть 1](https://github.com/deepmipt/tdl), [часть 2](https://github.com/deepmipt/tdl)). Я сам, увы, смотрел только пару лекций из него.

  Наверняка, этим занимаются и люди из Topological Data Analysis — науки про то, как с топологической точки зрения устроены многобразия, получаемые из данных.

  Мне представляется, что можно глянуть на это в разрезе задачи о стабильности нейросетей при внесении туда потери информации.

  Между слоями нейросети иногда добавляют Dropout-слой. Его задача — регуляризация весов. Для этого он зануляет выходы случайных нейронов, чтобы при обучении нейросеть не придавала слишком большое значение отдельным нейронам, а вместо этого «размазывала» содержательную информацию по нескольким нейронам ровным слоем.

  Интересна динамика этого процесса. Не на уровне итогового качества предсказания нейросети: это как раз хорошо изучено, а на уровне поведения отдельных нейронов и групп нейронов. Например, есть ли какие-то симметрии/инварианты в пространстве параметров, которые появляются в ходе такого дропаута (и которые были «спонтанно нарушены» при обучении без дропаута); как их находить, как их сохранять/устранять.

  Вот, например, такая пара исследовательских проектов:

  * Как ведёт себя обученная нейросеть, если добавить ей дропаут-слой и постепенно повышать dropout rate, начиная с очень низких значений? Можем ли мы следить за перетеканием весов? За изменением значимости нейронов? Как будет выглядеть «насыщение» доли выкинутых нейронов? Вопрос о том, выгоднее ли сразу учить с дропаутом или включать его на уже обученной сети, вроде решён в “[Curriculum Dropout](https://arxiv.org/abs/1703.06229)”. См. также “[Adaptive dropout for training deep neural networks](https://proceedings.neurips.cc/paper_files/paper/2013/file/7b5b23f4aadf9513306bcd59afb6e4c9-Paper.pdf)”.

    Известно также, что можно использовать dropout и pruning для дистилляции нейросети. Это статьи “[Self-Knowledge Distillation via Dropout](https://arxiv.org/pdf/2208.05642.pdf)” и “[Dropout distillation](https://www.researchgate.net/publication/305208757_Dropout_distillation)”

  * В рекуррентных сетях целью LSTM/GRU-ячеек является длительное сохранение важных аспектов состояния. RNN-ки обычно делают без дропаута, потому что если на каждой итерации терять информацию, то легко растерять всё (не уверен, но возможно это дополнительно усложняет вычисление градиентов).

    Тем не менее есть подходы, как применять дропаут в RNN-ках, см. [обзор](https://adriangcoder.medium.com/a-review-of-dropout-as-applied-to-rnns-72e79ecd5b7b).

    Мне же интересно, как будет распределять информацию сетка, чтобы избежать потери информации? Возможно ли это вообще с LSTM-ячейкой? с GRU-ячейкой? Можно взять простую задачу — типа вспомнить токен, который был N шагов назад. И посмотреть, как влияет дропаут на архитектуру сети.


* Эта задача у меня пока формулируется на уровне «размахивания руками». Мне нужно её додумать, чтобы выдать в качестве реального проекта.

  Сейчас довольно активно скрещивают графы знаний и графовые нейросети.

  Условно, у нас есть граф, где каждая вершина — это понятие, а ребро — тип связи. «Москва» → «столица» → «Россия»; «Москва» → «год основания» → 1147; «Москва» → «instance of» → «город»; «Россия» → «instance of» → «страна». Например, есть огромный свободно доступный граф знаний [Wikidata](https://wikidata.org).

  Теперь скажем, что каждая вершина (и, вероятно, каждое ребро) представлено некоторым эмбеддингом (векторным представлением). И теперь мы можем на графе строить короткие цепочки (трактуем их как что-то вроде предложений, где вершины и рёбра — это слова). И как в алгоритме word2vec, мы теперь можем скрывать отдельные токены и пытаться восстановить, что же было скрыто. Это позволяет нам учить наши эмбеддинги. А также предсказывать новые вершины и связи. Теперь мы можем в нашем графе спрашивать, а не надо ли поставить ребро между вершинами A и B, и ребро какого типа. То есть фактически можем автоматически пополнять наш граф знаний.

  А теперь к задаче, которой бы мне хотелось заняться. В текстовых методах у нас есть эмбеддинги целых предложений. Например, в seq2seq методах мы можем прочитать предложение слово за словом, закодировать его целиком в один вектор, а потом также слово за словом декодировать. Например, в текст на другом языке. Или в картинку. Или во что-то ещё.

  Теперь представьте, что я написал вопрос на естественном языке и закодировал его в вектор. Могу ли я теперь декодировать этот запрос в путь по графу. «В каком городе похоронен Пушкин?» → «Начни с вершины «Пушкин», пройди в обратную сторону по ребру «известные люди»; если таких рёбер несколько, выбери направление наиболее сонаправленное с «кладбище»/«похоронен» → пройди по ребру «населённый пункт»». И ответ: «Пушинские горы».

  Сходные маршруты по графам хотелось бы иметь для всевозможных алгоритмических применений. Тебя просят найти вершину графа, и дают путь с опечатками. Например, просят найти слово в суффиксном дереве, но в нём есть опечатки.


## Прикладное машинное обучение

* Слова-паразиты - алгоритм, который по аудиозаписи считает, сколько раз встретилось то или иное слово, не проводя промежуточной конвертации в текст.

  Промежуточная задача - алгоритм, который говорит, произнесено ли в двух аудиозаписях одно и то же слово или нет.

  Алгоритм может быть полезен для подсчета многомандатных выборов: там затруднительно считать бюллетени складыванием их в стопки, потому что на каждом несколько галок. Зато фамилию напротив каждой галочки озвучивают голосом, притом делает это один и тот же человек, что несколько упрощает задачу.

* (очень масштабная задача) Написать нейросеть, которая в режиме реального времени предсказывает, какой ракурс получит больше лайков - и таким образом помогает делать хорошие фото.

  Ключевая подзадача: спарсить огромное количество данных с инстаграма.


## Разное

* Генерировать и дешифровывать [автостереограммы](https://ru.wikipedia.org/wiki/Автостереограмма). То есть по картинке с точками получить карту глубин, которую увидел бы человек, расфокусировав зрение.

  Генераторы автостереограмм существуют. А вот расшифровка затруднительна. Некоторые проомежуточные успехи у нас были — на основе вычисления расстояний до ближайшей точки того же цвета.